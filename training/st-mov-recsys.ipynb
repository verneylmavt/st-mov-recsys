{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code sets the random seed for reproducibility across random, NumPy, and PyTorch operations. This ensures consistent results by fixing the seed for both CPU and GPU computations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code loads the MovieLens dataset by reading the ratings.csv and movies.csv files from the specified dataset path using Pandas.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../data/ml-32m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(os.path.join(dataset_path, 'ratings.csv'))\n",
    "movies = pd.read_csv(os.path.join(dataset_path, 'movies.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings: 32000204\n",
      "Number of users: 200948\n",
      "Number of movies: 84432\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of ratings: {ratings.shape[0]}')\n",
    "print(f'Number of users: {ratings.userId.nunique()}')\n",
    "print(f'Number of movies: {ratings.movieId.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code encodes the userId and movieId columns into numerical labels using LabelEncoder, which transforms categorical user and movie identifiers into integer indices. It also determines the number of unique users and movies.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['user'] = user_encoder.fit_transform(ratings['userId'])\n",
    "ratings['movie'] = movie_encoder.fit_transform(ratings['movieId'])\n",
    "\n",
    "num_users = ratings['user'].nunique()\n",
    "num_movies = ratings['movie'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code splits the ratings data into training, validation, and test sets using train_test_split. It first splits off 10% of the data for testing and then splits the remaining data into training and validation sets.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(ratings, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1111, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Train Set: {train.shape[0]} samples')\n",
    "# print(f'Validation Set: {val.shape[0]} samples')\n",
    "# print(f'Test Set: {test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a custom Dataset class named MovieLensDataset that facilitates the retrieval of user, movie, and rating data samples. This class is essential for creating data loaders that can feed data into the model during training and evaluation. Then, it creates instances of the MovieLensDataset for training, validation, and testing. Lastly, it wraps these datasets in DataLoader objects to enable efficient batching and shuffling of data during the training and evaluation processes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.users = dataframe['user'].values\n",
    "        self.movies = dataframe['movie'].values\n",
    "        self.ratings = dataframe['rating'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        movie = self.movies[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        return user, movie, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieLensDataset(train)\n",
    "val_dataset = MovieLensDataset(val)\n",
    "test_dataset = MovieLensDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_iter = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the NCF class, a neural network model for collaborative filtering. The model consists of embedding layers for users and movies, followed by a multilayer perceptron (MLP) with ReLU activations and dropout for regularization. The final output layer predicts the rating by producing a single scalar value.\n",
    "\n",
    "• User and Item Embeddings\n",
    "The embedding layers represent users and items (movies in this context) as dense vectors in a latent space of fixed dimensionality (embedding_dim). These embeddings capture unique characteristics of each user and item based on their interaction patterns. The embeddings are initialized with small random values (via nn.init.normal_) to start training effectively. As the model learns, these embeddings are updated to encode meaningful latent features relevant to predicting user preferences.\n",
    "\n",
    "• Concatenation of User and Item Embeddings\n",
    "After retrieving the user and item embeddings, the model concatenates them along the last dimension. This operation merges the user and item information into a single vector, serving as the input to the MLP. This concatenated representation allows the MLP to learn the joint interaction between user and item features.\n",
    "\n",
    "• Multi-Layer Perceptron (MLP)\n",
    "The MLP in the NCF model captures complex, non-linear relationships between users and items:\n",
    "- Structure:\n",
    "The input to the MLP is the concatenated embeddings of users and items.\n",
    "The MLP is composed of fully connected (Linear) layers, interleaved with ReLU activations and dropout.\n",
    "- Hidden Layers:\n",
    "Each hidden layer progressively transforms the input, learning higher-order feature interactions.\n",
    "The number and size of the hidden layers are configurable via the hidden_layers parameter.\n",
    "- Dropout Regularization:\n",
    "Dropout layers prevent overfitting by randomly deactivating neurons during training, ensuring the MLP generalizes well to unseen data.\n",
    "\n",
    "• Output Layer\n",
    "The final layer of the model is a linear layer that outputs a single scalar value. This value represents the predicted interaction strength or rating between the user and item. The absence of an activation function in the output layer allows the model to predict continuous values (e.g., ratings) or logits (for binary tasks).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50, hidden_layers=[128, 64], dropout=0.2):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        # Embedding Layer for User Representations\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        # Embedding Layer for Item Representations\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        # Weight Initialization for User Embedding\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        # Weight Initialization for Item Embedding\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "        # Sequential Model for Multi-Layer Perceptron (MLP)\n",
    "        layers = []\n",
    "        # Input Size for MLP (Concatenated User and Item Embeddings)\n",
    "        input_size = embedding_dim * 2\n",
    "        # Hidden Layers Construction for MLP\n",
    "        for hidden in hidden_layers:\n",
    "            # Fully Connected Layer for MLP\n",
    "            layers.append(nn.Linear(input_size, hidden))\n",
    "            # Activation Layer for Non-Linear Transformations\n",
    "            layers.append(nn.ReLU())\n",
    "            # Dropout Layer for Regularization\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            # Update Input Size for Next Layer\n",
    "            input_size = hidden\n",
    "        # MLP Network for Learning User-Item Interactions\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output Layer for Predicting Ratings\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        # Embedding of Input Users\n",
    "        user_emb = self.user_embedding(user)\n",
    "        # Embedding of Input Items\n",
    "        item_emb = self.item_embedding(item)\n",
    "        # Concatenation of User and Item Embeddings\n",
    "        x = torch.cat([user_emb, item_emb], dim=-1)\n",
    "        # Transformation of Concatenated Embeddings w/ MLP\n",
    "        x = self.mlp(x)\n",
    "        # Transformation of MLP Output → Predicted Ratings\n",
    "        rating = self.output_layer(x)\n",
    "        # Squeezing of Output Ratings\n",
    "        return rating.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the train_epoch function, which handles the training process for one epoch. It iterates over the training data loader, performs forward passes, computes the loss, backpropagates the gradients, updates the model parameters, and accumulates the running loss and Mean Absolute Error (MAE) for monitoring.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, optimizer, criterion, device):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    for users, movies, ratings_batch in train_iter:\n",
    "        users = users.to(device, dtype=torch.long)\n",
    "        movies = movies.to(device, dtype=torch.long)\n",
    "        ratings_batch = ratings_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(users, movies)\n",
    "        loss = criterion(outputs, ratings_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * users.size(0)\n",
    "        running_mae += torch.abs(outputs - ratings_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_iter.dataset)\n",
    "    epoch_mae = running_mae / len(train_iter.dataset)\n",
    "    return epoch_loss, epoch_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the evaluate_epoch function, which evaluates the model's performance on the validation set. It computes the loss and MAE without updating the model parameters, allowing for monitoring of the model's generalization ability.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(net, val_iter, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings_batch in val_iter:\n",
    "            users = users.to(device, dtype=torch.long)\n",
    "            movies = movies.to(device, dtype=torch.long)\n",
    "            ratings_batch = ratings_batch.to(device)\n",
    "\n",
    "            outputs = net(users, movies)\n",
    "            loss = criterion(outputs, ratings_batch)\n",
    "\n",
    "            running_loss += loss.item() * users.size(0)\n",
    "            running_mae += torch.abs(outputs - ratings_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_iter.dataset)\n",
    "    epoch_mae = running_mae / len(val_iter.dataset)\n",
    "    return epoch_loss, epoch_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code implements the main training loop that runs for a specified number of epochs. For each epoch, it trains the model using the training data, evaluates it on the validation data, and records the training and validation losses and accuracies. It also incorporates early stopping by monitoring the validation loss and saving the best model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "hidden_layers = [128, 64]\n",
    "dropout = 0.2\n",
    "net = NCF(num_users, num_movies, embedding_dim, hidden_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    print(f\"epoch {epoch}/{num_epochs}\")\n",
    "    \n",
    "    epoch_loss, epoch_mae = train_epoch(net, train_iter, optimizer, criterion, device)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_maes.append(epoch_mae)\n",
    "    \n",
    "    val_epoch_loss, val_epoch_mae = evaluate_epoch(net, val_iter, criterion, device)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_maes.append(val_epoch_mae)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    print(f\"train loss: {epoch_loss:.4f}, train mae: {epoch_mae:.4f}, val loss: {val_epoch_loss:.4f}, val mae: {val_epoch_mae:.4f}, time: {epoch_time:.2f}s\")\n",
    "\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(net.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the cal_metrics function, which evaluates the trained model on the test dataset. It calculates and prints evaluation metrics including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) to assess the model's performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_37628\\61487756.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('best_ncf_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metrics(net, test_iter):\n",
    "    net.eval()\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings_batch in test_iter:\n",
    "            users = users.to(device, dtype=torch.long)\n",
    "            movies = movies.to(device, dtype=torch.long)\n",
    "            ratings_batch = ratings_batch.to(device)\n",
    "\n",
    "            outputs = net(users, movies)\n",
    "            test_predictions.extend(outputs.cpu().numpy())\n",
    "            test_targets.extend(ratings_batch.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_targets = np.array(test_targets)\n",
    "\n",
    "    test_mse = mean_squared_error(test_targets, test_predictions)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(test_targets, test_predictions)\n",
    "    \n",
    "    print(f'MSE: {test_mse:.4f}')\n",
    "    print(f'RMSE: {test_rmse:.4f}')\n",
    "    print(f'MAE: {test_mae:.4f}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.6059\n",
      "RMSE: 0.7784\n",
      "MAE: 0.5853\n"
     ]
    }
   ],
   "source": [
    "cal_metrics(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-Based Recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the recommend_movies function, which generates movie recommendations for an existing user by predicting ratings for movies the user hasn't rated yet. It encodes the user ID, filters out already rated movies, predicts ratings using the trained model, and selects the top-rated movies to recommend.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, net, movies_df, user_encoder, movie_encoder, ratings_original, num_recommendations=10):\n",
    "    net.eval()\n",
    "    user_encoded = user_encoder.transform([user_id])[0]\n",
    "    \n",
    "    all_movies = np.arange(num_movies)\n",
    "    user_data = ratings_original[ratings_original['userId'] == user_id]\n",
    "    rated_movie_ids = user_data['movieId'].unique()\n",
    "    rated_movies = movie_encoder.transform(rated_movie_ids)\n",
    "    movies_to_predict = np.setdiff1d(all_movies, rated_movies)\n",
    "    \n",
    "    user_tensor = torch.tensor([user_encoded] * len(movies_to_predict)).to(device, dtype=torch.long)\n",
    "    movie_tensor = torch.tensor(movies_to_predict).to(device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = net(user_tensor, movie_tensor).cpu().numpy()\n",
    "    \n",
    "    top_indices = predictions.argsort()[-num_recommendations:][::-1]\n",
    "    top_movie_indices = movies_to_predict[top_indices]\n",
    "    top_movie_ids_original = movie_encoder.inverse_transform(top_movie_indices)\n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(top_movie_ids_original)][['movieId', 'title']]\n",
    "    \n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       movieId                                title\n",
      "897        918          Meet Me in St. Louis (1944)\n",
      "906        927                    Women, The (1939)\n",
      "1012      1035           Sound of Music, The (1965)\n",
      "6961      7085            Send Me No Flowers (1964)\n",
      "18006    93988                 North & South (2004)\n",
      "24303   121097  To Grandmother's House We Go (1992)\n",
      "27480   129032           Sense & Sensibility (2008)\n",
      "27729   129788                    Raanjhanaa (2013)\n",
      "33586   142929                         Sissi (1955)\n",
      "77263   256991      Adventure Time: Elements (2017)\n"
     ]
    }
   ],
   "source": [
    "recommended = recommend_movies(2, net, movies, user_encoder, movie_encoder, ratings, 10)\n",
    "print(recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Extraction and Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "print(movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp  user  movie\n",
      "0       1       17     4.0  944249077     0     16\n",
      "1       1       25     1.0  944250228     0     24\n",
      "2       1       29     2.0  943230976     0     28\n",
      "3       1       30     5.0  944249077     0     29\n",
      "4       1       32     5.0  943228858     0     31\n"
     ]
    }
   ],
   "source": [
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movie Embeddings Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the get_movie_embeddings function, which retrieves the movie embeddings from the trained model. It normalizes these embeddings to unit vectors, facilitating the computation of cosine similarities between movies.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_embeddings(net):\n",
    "    movie_embeddings = net.item_embedding.weight.data.cpu().numpy()\n",
    "    movie_embeddings_normalized = movie_embeddings / np.linalg.norm(movie_embeddings, axis=1, keepdims=True)\n",
    "    return movie_embeddings_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embeddings_normalized = get_movie_embeddings(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine Similarity Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the compute_cosine_similarity function, which calculates the cosine similarity matrix for the provided embeddings. This matrix quantifies the similarity between each pair of movie embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(embeddings):\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Genre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code processes the movie genres by splitting the genre strings, encoding them into multi-hot vectors, and appending these encoded genres to the movies_inf DataFrame. This allows the model to consider genre information during recommendations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_inf = movies.copy()\n",
    "ratings_inf = ratings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_inf['genres'] = movies_inf['genres'].apply(lambda x: [genre.strip() for genre in x.split('|')])\n",
    "all_genres = sorted(set(genre for sublist in movies_inf['genres'] for genre in sublist if genre != '(no genres listed)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hot_encode(genres, all_genres):\n",
    "    return [1 if genre in genres else 0 for genre in all_genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_inf['genres_encoded'] = movies_inf['genres'].apply(lambda x: multi_hot_encode(x, all_genres))\n",
    "genres_matrix = np.vstack(movies_inf['genres_encoded'].values)\n",
    "genres_df = pd.DataFrame(genres_matrix, columns=all_genres)\n",
    "movies_inf = pd.concat([movies_inf, genres_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity-Based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the recommend_similar_movies function, which recommends movies similar to a given movie based on cosine similarity of their embeddings. It ensures the input movie exists, retrieves its embedding, computes similarities with all other movies, and selects the top similar movies to recommend.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_movies(movie_id, movies_df, movie_encoder, movie_embeddings, top_n=10):\n",
    "    if movie_id not in movies_df['movieId'].values:\n",
    "        print(\"Movie ID not found in the dataset.\")\n",
    "        return pd.DataFrame()\n",
    "    movie_encoded = movie_encoder.transform([movie_id])[0]\n",
    "    \n",
    "    target_embedding = movie_embeddings[movie_encoded].reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(target_embedding, movie_embeddings).flatten()\n",
    "    similar_indices = similarities.argsort()[-(top_n + 1):-1][::-1]\n",
    "    similar_movie_ids = movie_encoder.inverse_transform(similar_indices)\n",
    "    \n",
    "    recommended_movies = movies_df[movies_df['movieId'].isin(similar_movie_ids)][['movieId', 'title']]\n",
    "    recommended_movies = recommended_movies.copy()\n",
    "    recommended_movies['similarity'] = similarities[similar_indices]\n",
    "    \n",
    "    return recommended_movies.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                    title  similarity\n",
      "0      588           Aladdin (1992)    0.949216\n",
      "1     2355     Bug's Life, A (1998)    0.893583\n",
      "2     3114       Toy Story 2 (1999)    0.873036\n",
      "3     4886    Monsters, Inc. (2001)    0.868784\n",
      "4     6377      Finding Nemo (2003)    0.839115\n",
      "5     8961  Incredibles, The (2004)    0.800943\n",
      "6    50872       Ratatouille (2007)    0.789139\n",
      "7    60069            WALL·E (2008)    0.777917\n",
      "8    68954                Up (2009)    0.760231\n",
      "9    78499       Toy Story 3 (2010)    0.737283\n"
     ]
    }
   ],
   "source": [
    "movie_id_example = movies_inf['movieId'].iloc[0]\n",
    "similar_movies = recommend_similar_movies(movie_id_example, movies_inf, movie_encoder, movie_embeddings_normalized, top_n=10)\n",
    "print(similar_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genre-Based Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the recommend_by_genre function, which recommends movies based on specified genres. It filters movies that match all provided genres, calculates their popularity based on rating counts, sorts them by popularity, and returns the top N recommendations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_genre(genres, movies_df, ratings_original, top_n=10):\n",
    "    valid_genres = sorted(all_genres)\n",
    "    for genre in genres:\n",
    "        if genre not in valid_genres:\n",
    "            print(f\"Genre '{genre}' is not recognized. Valid genres are: {valid_genres}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    filtered_movies = movies_df\n",
    "    for genre in genres:\n",
    "        filtered_movies = filtered_movies[filtered_movies[genre] == 1]\n",
    "    \n",
    "    if filtered_movies.empty:\n",
    "        print(\"No movies found with the specified genres.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    popularity = ratings_original.groupby('movieId').size().reset_index(name='rating_count')\n",
    "    recommended = filtered_movies.merge(popularity, on='movieId', how='left').fillna(0)\n",
    "    recommended = recommended.sort_values(by='rating_count', ascending=False)\n",
    "    recommended = recommended[['movieId', 'title', 'rating_count']].head(top_n)\n",
    "    return recommended.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                                              title  rating_count\n",
      "0      590                          Dances with Wolves (1990)       46771.0\n",
      "1    99114                            Django Unchained (2012)       32043.0\n",
      "2     2012                 Back to the Future Part III (1990)       26166.0\n",
      "3     1201  Good, the Bad and the Ugly, The (Buono, il bru...       22922.0\n",
      "4      368                                    Maverick (1994)       18685.0\n",
      "5      266                         Legends of the Fall (1994)       17820.0\n",
      "6     1266                                  Unforgiven (1992)       17593.0\n",
      "7     1304          Butch Cassidy and the Sundance Kid (1969)       17587.0\n",
      "8      163                                   Desperado (1995)       16327.0\n",
      "9      553                                   Tombstone (1993)       16211.0\n"
     ]
    }
   ],
   "source": [
    "genres_example = ['Comedy']\n",
    "genre_based_movies = recommend_by_genre(genres_example, movies_inf, ratings_inf, top_n=10)\n",
    "print(genre_based_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the recommend_combined function, which generates movie recommendations by combining similarity-based and genre-based approaches. It ensures the input movie exists, retrieves similar movies, filters them based on specified genres, incorporates popularity metrics, and returns the top N combined recommendations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_combined(movie_id, genres, net, movies_df, movie_encoder, movie_embeddings, ratings_original, top_n=10):\n",
    "\n",
    "    if movie_id not in movies_df['movieId'].values:\n",
    "        print(\"Movie ID not found in the dataset.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    movie_encoded = movie_encoder.transform([movie_id])[0]\n",
    "    \n",
    "    target_embedding = movie_embeddings[movie_encoded].reshape(1, -1)\n",
    "    \n",
    "    similarities = cosine_similarity(target_embedding, movie_embeddings).flatten()\n",
    "    similar_indices = similarities.argsort()[-(top_n * 2 + 1):-1][::-1]\n",
    "    similar_movie_ids = movie_encoder.inverse_transform(similar_indices)\n",
    "    similar_movies = movies_df[movies_df['movieId'].isin(similar_movie_ids)].copy()\n",
    "    similar_movies['similarity'] = similarities[similar_indices]\n",
    "    \n",
    "    for genre in genres:\n",
    "        if genre in all_genres:\n",
    "            similar_movies = similar_movies[similar_movies[genre] == 1]\n",
    "        else:\n",
    "            print(f\"Genre '{genre}' is not recognized.\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    if similar_movies.empty:\n",
    "        print(\"No similar movies found with the specified genres.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    popularity = ratings_original.groupby('movieId').size().reset_index(name='rating_count')\n",
    "    \n",
    "    similar_movies = similar_movies.merge(popularity, on='movieId', how='left').fillna(0)\n",
    "    similar_movies = similar_movies.sort_values(by=['similarity', 'rating_count'], ascending=[False, False])\n",
    "    \n",
    "    recommended = similar_movies[['movieId', 'title', 'similarity', 'rating_count']].head(top_n)\n",
    "    \n",
    "    return recommended.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                            title  similarity  rating_count\n",
      "0     1722       Tomorrow Never Dies (1997)    0.765966         14288\n",
      "1     2947                Goldfinger (1964)    0.718134         14422\n",
      "2     2948     From Russia with Love (1963)    0.705448          8772\n",
      "3     2949                    Dr. No (1962)    0.701961          8898\n",
      "4     2989        For Your Eyes Only (1981)    0.687727          4933\n",
      "5     2990           Licence to Kill (1989)    0.674686          4816\n",
      "6     2991          Live and Let Die (1973)    0.671496          5787\n",
      "7     2993               Thunderball (1965)    0.661628          5299\n",
      "8     3082  World Is Not Enough, The (1999)    0.657112         10496\n",
      "9     3635     Spy Who Loved Me, The (1977)    0.652205          5185\n"
     ]
    }
   ],
   "source": [
    "movie_id_example_combined = 10\n",
    "genres_example_combined = ['Action', 'Adventure']\n",
    "combined_recommended = recommend_combined(\n",
    "    movie_id_example_combined,\n",
    "    genres_example_combined,\n",
    "    net,\n",
    "    movies_inf,\n",
    "    movie_encoder,\n",
    "    movie_embeddings_normalized,\n",
    "    ratings_inf,\n",
    "    top_n=10\n",
    ")\n",
    "print(combined_recommended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
